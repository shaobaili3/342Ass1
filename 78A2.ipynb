{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "78A2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPcd7T+yEkfVvAWvWAblUoo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaobaili3/342Ass1/blob/master/78A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paGZaencfYj_",
        "colab_type": "text"
      },
      "source": [
        "# Retrieve Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbTUy73aqeVJ",
        "colab_type": "text"
      },
      "source": [
        "## Link Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5xMDy1t7v5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "547ea582-e49a-4961-a4a4-abb295e25d4b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaxEQQri8C6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v_path = \"/content/drive/My Drive/NLPa2/v.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3kO1STUqcRG",
        "colab_type": "text"
      },
      "source": [
        "## Parse dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRy86W4Sfkhx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ce194062-c8ab-40e3-c065-fbcb67bd763c"
      },
      "source": [
        "train_path = \"/content/drive/My Drive/NLPa2/train.csv\"\n",
        "test_path = \"/content/drive/My Drive/NLPa2/test.csv\"\n",
        "val_path = \"/content/drive/My Drive/NLPa2/val.csv\" #3000 in total\n",
        "result_path = \"/content/drive/My Drive/NLPa2/result.csv\"\n",
        "\n",
        "def read_data(file_name, n_sample):\n",
        "    f = open(file_name)\n",
        "    documents = f.readlines()\n",
        "    documents.pop(0) # Remove title, i.e., sentence, NER\n",
        "    input_data = []\n",
        "\n",
        "    target_data = []\n",
        "\n",
        "    for i in documents[0: n_sample]:\n",
        "      temp_words = []\n",
        "      temp_labels = []\n",
        "      line = i.replace('\\n','').rsplit(',', 1)\n",
        "      input = line[0]\n",
        "      target = line[1]\n",
        "      \n",
        "      for word in input.split():\n",
        "        temp_words.append(word)\n",
        "      for label in target.split():\n",
        "        temp_labels.append(label)\n",
        "      \n",
        "      input_data.append(temp_words)\n",
        "      target_data.append(temp_labels)\n",
        "\n",
        "    return input_data, target_data\n",
        "\n",
        "def get_test_strings():\n",
        "  test_data = []\n",
        "  test_string = []\n",
        "  f = open(test_path)\n",
        "  documents = f.readlines()\n",
        "  documents.pop(0) # Remove title, i.e., sentence, NER\n",
        "  for i in documents:\n",
        "    words = i.replace('\\n','')[:-1]\n",
        "    test_string.append(words)\n",
        "    temp = []\n",
        "    for word in words.split():\n",
        "    \ttemp.append(word)\n",
        "    test_data.append(temp)\n",
        "  return test_data, test_string\n",
        "\n",
        "train_data, target_y_train = read_data(train_path, 3000) # 3000 in total\n",
        "validation_data, target_y_validation = read_data(val_path, 700)\n",
        "#test_data, target_y_test = read_data(val_path,700)\n",
        "test_data , test_string = get_test_strings()\n",
        "\n",
        "print(\"train_data\", len(train_data))\n",
        "print(\"validation_data\", len(validation_data))\n",
        "print(\"test_data\", len(test_data))\n",
        "\n",
        "# print(train_data[-1])\n",
        "# print(target_y_train[0:5])\n",
        "# print(validation_data[0:5])\n",
        "# print(target_y_validation[0:5])\n",
        "# print(test_data[0:5])\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_data 3000\n",
            "validation_data 700\n",
            "test_data 3684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcKFRmCg0YNd",
        "colab_type": "text"
      },
      "source": [
        "## Generate index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYUlFQC_0cSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get word_to_ix and tag_to_ix\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "\n",
        "def get_index(all_data, all_target):\n",
        "  word_to_ix = {}\n",
        "  for sentence in all_data:\n",
        "      for word in sentence:\n",
        "          word = word.lower()\n",
        "          if word not in word_to_ix:\n",
        "              word_to_ix[word] = len(word_to_ix)\n",
        "  word_list = list(word_to_ix.keys())\n",
        "  tag_to_ix = {START_TAG:0, STOP_TAG:1}\n",
        "  for tags in all_target:\n",
        "      for tag in tags:\n",
        "          if tag not in tag_to_ix:\n",
        "              tag_to_ix[tag] = len(tag_to_ix)\n",
        "  return word_to_ix, tag_to_ix, word_list\n",
        "\n",
        "# Get index from data\n",
        "def to_index(data, to_ix):\n",
        "  input_index_list = []\n",
        "  for sent in data:\n",
        "    input_index_list.append([to_ix[w] for w in sent])\n",
        "  return input_index_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BcL2Mt50sFr",
        "colab_type": "text"
      },
      "source": [
        "# Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ3t18Pu0rvN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "eea7c9d7-dff6-45cd-ad8d-5dfcc42749c2"
      },
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "\n",
        "word_emb_model = api.load(\"glove-twitter-200\") \n",
        "\n",
        "def get_embedding_matrix(EMBEDDING_DIM):\n",
        "  embedding_matrix = []\n",
        "  for word in word_list:\n",
        "    try:\n",
        "        embedding_matrix.append(word_emb_model.wv[word])\n",
        "    except:\n",
        "        embedding_matrix.append([0]*EMBEDDING_DIM)\n",
        "  embedding_matrix = np.array(embedding_matrix)\n",
        "  embedding_matrix.shape\n",
        "  return embedding_matrix"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9BC5Esu0H3Q",
        "colab_type": "text"
      },
      "source": [
        "# BI-LSTM baseline model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgimKOdb7SCv",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ldw2BnQZ0SYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, embedding_matrix):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq\n",
        "\n",
        "def initialize_model(EMBEDDING_DIM, HIDDEN_DIM, embedding_matrix):\n",
        "  model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, embedding_matrix).to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "  return model, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v7Ahmfn7XJe",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plaXHM-L0Ih-",
        "colab_type": "text"
      },
      "source": [
        "# Validation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zgCe2Rw3-JR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def cal_acc(model, input_index, output_index):\n",
        "  predicts = []\n",
        "  totalNum = 0 \n",
        "  correctNum = 0\n",
        "  for i, idxs in enumerate(input_index):\n",
        "    sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "    predict = model(sentence_in)[1]\n",
        "    predicts.append(predict)\n",
        "    output = output_index[i]\n",
        "    for i, v in enumerate(predict):\n",
        "      totalNum += 1\n",
        "      if v == output[i]:\n",
        "        correctNum += 1\n",
        "  first = []\n",
        "  second = []\n",
        "  for a in output_index:\n",
        "    for o in a:\n",
        "      first.append(o)\n",
        "  for a in predicts:\n",
        "    for o in a:\n",
        "      second.append(o)\n",
        "  return first, second, correctNum/totalNum\n",
        "\n",
        "\n",
        "def get_result(model, input_index):\n",
        "  predicts = []\n",
        "  for i, idxs in enumerate(input_index):\n",
        "    sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "    predict = model(sentence_in)[1]\n",
        "    predicts.append(predict)\n",
        "  return predicts\n",
        "\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNlt8Qmu0IrI",
        "colab_type": "text"
      },
      "source": [
        "# Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0kfHdEj5LJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_result(results):\n",
        "  test_results = []\n",
        "  for result in results:\n",
        "    test_result = decode_output(result)\n",
        "    test_results.append(test_result)\n",
        "  return test_results\n",
        "\n",
        "def write_csv(result):\n",
        "  f = open(result_path, \"w\")\n",
        "  f.write(\"Id,Predicted\\n\")\n",
        "  index = 0\n",
        "  for r in result:\n",
        "    for c in r:\n",
        "      f.write(str(index) + \",\" + c + \"\\n\")\n",
        "      index += 1\n",
        "  f.close()\n",
        "  print(\"Result Updated in Drive!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAk5DJLI52Mg",
        "colab_type": "text"
      },
      "source": [
        "# Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk_EVjhw_rn8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "91965210-ce34-4a89-dd54-a0c261468421"
      },
      "source": [
        "# Data retrieval\n",
        "train_data, target_y_train = read_data(train_path, 3000) # 3000 in total\n",
        "validation_data, target_y_validation = read_data(val_path, 700)\n",
        "#test_data, target_y_test = read_data(val_path,700)\n",
        "test_data , test_string = get_test_strings()\n",
        "\n",
        "all_data = train_data + validation_data + test_data\n",
        "all_target = target_y_train + target_y_validation\n",
        "word_to_ix, tag_to_ix, word_list = get_index(all_data, all_target)\n",
        "\n",
        "train_input_index =  to_index(train_data,word_to_ix)\n",
        "train_output_index = to_index(target_y_train,tag_to_ix)\n",
        "val_input_index = to_index(validation_data,word_to_ix)\n",
        "val_output_index = to_index(target_y_validation,tag_to_ix)\n",
        "test_input_index = to_index(test_data,word_to_ix)\n",
        "#test_output_index = to_index(target_y_test,tag_to_ix)\n",
        "\n",
        "EMBEDDING_DIM = 200\n",
        "HIDDEN_DIM = 200\n",
        "embedding_matrix = get_embedding_matrix(EMBEDDING_DIM)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVbiCA9SIr-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize model\n",
        "model, optimizer = initialize_model(EMBEDDING_DIM, HIDDEN_DIM, embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kojXcWLAu82",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE3SM10z6xLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "\n",
        "def train(model, epoch_size):\n",
        "  bestScore = 0.93\n",
        "  for epoch in range(epoch_size):  #20\n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for i, idxs in enumerate(train_input_index):\n",
        "        tags_index = train_output_index[i]\n",
        "\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n",
        "    #_, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n",
        "    _, _, val_acc = cal_acc(model,v_input_index,v_output_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i, idxs in enumerate(val_input_index):\n",
        "        tags_index = val_output_index[i]\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "    if val_acc > bestScore:\n",
        "      bestScore = val_acc\n",
        "      print(\"best!!!: \" , bestScore)\n",
        "      torch.save(model, \"/content/drive/My Drive/NLPa2/model.pt\")\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m73u464r_SNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def get_kaggle():\n",
        "  test_input_index = to_index(test_data, word_to_ix)\n",
        "  results = get_result(model, test_input_index)\n",
        "  write_csv(decode_result(results))\n",
        "\n",
        "def validate():\n",
        "  y_true,y_pred,_ = cal_acc(model, val_input_index, val_output_index)\n",
        "  y_true_decode = decode_output(y_true)\n",
        "  y_pred_decode = decode_output(y_pred)\n",
        "  print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRKpmKmTTLvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train(model, 100)\n",
        "# validate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra6t_XxDqMW3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "541f1378-64b3-4783-825d-a2cee3abe075"
      },
      "source": [
        "v_data, target_y_v = read_data(v_path, 36854)\n",
        "v_input_index = to_index(v_data,word_to_ix)\n",
        "v_output_index = to_index(target_y_v,tag_to_ix)\n",
        "\n",
        "def validate2():\n",
        "  y_true,y_pred,_ = cal_acc(model, v_input_index, v_output_index)\n",
        "  y_true_decode = decode_output(y_true)\n",
        "  y_pred_decode = decode_output(y_pred)\n",
        "  print(classification_report(y_true_decode,y_pred_decode,digits=4))\n",
        "\n",
        "validate2()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       I-LOC     0.0351    0.4390    0.0650      1925\n",
            "      I-MISC     0.0000    0.0000    0.0000       918\n",
            "       I-ORG     0.2043    0.0188    0.0345      2496\n",
            "       I-PER     0.0000    0.0000    0.0000      2773\n",
            "           O     0.8074    0.4685    0.5929     38554\n",
            "\n",
            "    accuracy                         0.4062     46666\n",
            "   macro avg     0.2094    0.1853    0.1385     46666\n",
            "weighted avg     0.6794    0.4062    0.4944     46666\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxdFU8EYBB_Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d55c433c-4c60-49ae-b6d2-7f7acd6f799f"
      },
      "source": [
        "# train(model, 200) \n",
        "model = torch.load(\"/content/drive/My Drive/NLPa2/model.pt\")\n",
        "model.eval()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM_CRF(\n",
              "  (word_embeds): Embedding(14882, 200)\n",
              "  (lstm): LSTM(200, 200, bidirectional=True)\n",
              "  (hidden2tag): Linear(in_features=400, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVBTdfjsXdOI",
        "colab_type": "text"
      },
      "source": [
        "SUBMISSION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrkXQx8YTxu6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "31ccb1b1-064c-44b0-f45d-cdb970de2029"
      },
      "source": [
        "get_kaggle()\n",
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp /content/drive/My\\ Drive/NLPa2/kaggle.json ~/.kaggle/\n",
        "!kaggle competitions submit -c 2020-comp5046-a2 -f /content/drive/My\\ Drive/NLPa2/result.csv -m \"test\""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Result Updated in Drive!\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "100% 384k/384k [00:01<00:00, 337kB/s]\n",
            "Successfully submitted to COMP5046-2020S1-A2"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}